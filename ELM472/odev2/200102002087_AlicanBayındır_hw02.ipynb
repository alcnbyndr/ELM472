{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alican Bayındır\n",
    "# ELM 472 - ODEV2\n",
    "# Naive Bayes ile spam mail detection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "      <th>label_nums</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                               text  label_nums\n",
       "0    ham  Subject: enron methanol ; meter # : 988291\\r\\n...           0\n",
       "1    ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...           0\n",
       "2    ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...           0\n",
       "3   spam  Subject: photoshop , windows , office . cheap ...           1\n",
       "4    ham  Subject: re : indian springs\\r\\nthis deal is t...           0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam_ham_dataset.csv', encoding='ISO-8859-1')\n",
    "df = df.drop([\"Unnamed: 0\"], axis=\"columns\")\n",
    "df.columns = ['labels', 'text','label_nums']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3878, 3)\n",
      "(1293, 3)\n"
     ]
    }
   ],
   "source": [
    "# datayı test ve training için ayırma\n",
    "data_randomized = df.sample(frac=1, random_state=1)\n",
    "training_test_index = round(len(data_randomized) * 0.75)\n",
    "training_set = data_randomized[:training_test_index].reset_index(drop=True)\n",
    "test_set = data_randomized[training_test_index:].reset_index(drop=True)\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alican\\AppData\\Local\\Temp/ipykernel_1008/3722485601.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  training_set['text'] = training_set['text'].str.replace('\\W', ' ') # Removes punctuation\n"
     ]
    }
   ],
   "source": [
    "# datada tekrar eden alakasız kelimeleri silme\n",
    "training_set['text'] = training_set['text'].str.replace('\\W', ' ') # Removes punctuation\n",
    "training_set['text'] = training_set['text'].replace('\\r'  , ' ')\n",
    "training_set['text'] = training_set['text'].replace(';'   , ' ')\n",
    "training_set['text'] = training_set['text'].replace('.'   , ' ')\n",
    "training_set['text'] = training_set['text'].replace('\\''  , ' ')\n",
    "training_set['text'] = training_set['text'].replace(':'   , ' ')\n",
    "training_set['text'] = training_set['text'].replace('\\n'  , ' ')\n",
    "training_set['text'] = training_set['text'].replace('#'   , ' ')\n",
    "training_set['text'] = training_set['text'].replace(\"/\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"-\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"/\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"|\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\",\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\">\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"|\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"(\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\")\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"[\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"]\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"1\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"2\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"3\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"4\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"5\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"6\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"7\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"8\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"9\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"0\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"    \", \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"  \"  , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"  \"  , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"+\"   , \" \")\n",
    "training_set['text'] = training_set['text'].replace(\"we ' re\"     , \"we are\")\n",
    "training_set['text'] = training_set['text'].replace(\"they ' re\",\"they are\")\n",
    "training_set['text'] = training_set['text'].replace(\"you ' re\",\"you are\")\n",
    "training_set['text'] = training_set['text'].replace(\"Subject\",\" \")\n",
    "training_set['text'] = training_set['text'].str.lower()\n",
    "training_set['text'] = training_set['text'].str.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WholeWords = []\n",
    "for data in training_set['text']:\n",
    "   for word in data:\n",
    "      WholeWords.append(word)\n",
    "\n",
    "WholeWords = list(set(WholeWords))\n",
    "TotalNumberOfWords = len(WholeWords)\n",
    "WordsPerData = {unique_word: [0] * len(training_set['text']) for unique_word in WholeWords}\n",
    "\n",
    "for i, data in enumerate(training_set['text']):\n",
    "   for j in data:\n",
    "      WordsPerData[j][i] += 1\n",
    "\n",
    "WordCounts = pd.DataFrame(WordsPerData)\n",
    "\n",
    "training_set_clean = pd.concat([training_set, WordCounts], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam olasılıklarının hesabı\n",
    "SpamMessages = training_set_clean[training_set_clean['labels'] == 'spam']\n",
    "HamMessages = training_set_clean[training_set_clean['labels'] == 'ham']\n",
    "\n",
    "ProbabilitySpam = len(SpamMessages) / len(training_set_clean)\n",
    "ProbabilityHam = len(HamMessages) / len(training_set_clean)\n",
    "\n",
    "n_words_per_spam_message = SpamMessages['text'].apply(len)\n",
    "n_spam = n_words_per_spam_message.sum()\n",
    "\n",
    "n_words_per_ham_message = HamMessages['text'].apply(len)\n",
    "n_ham = n_words_per_ham_message.sum()\n",
    "\n",
    "n_vocabulary = len(WholeWords)\n",
    "\n",
    "parameters_spam = {unique_word:0 for unique_word in WholeWords}\n",
    "parameters_ham = {unique_word:0 for unique_word in WholeWords}\n",
    "\n",
    "#her bir kelime için olasılık hesabı\n",
    "for word in WholeWords:\n",
    "   n_word_given_spam = SpamMessages[word].sum()\n",
    "   p_word_given_spam = (n_word_given_spam + 1) / (n_spam + n_vocabulary)\n",
    "   parameters_spam[word] = p_word_given_spam\n",
    "\n",
    "   n_word_given_ham = HamMessages[word].sum()\n",
    "   p_word_given_ham = (n_word_given_ham + 1) / (n_ham + n_vocabulary)\n",
    "   parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "   message = re.sub('\\W', ' ', message)\n",
    "   message = message.lower().split()\n",
    "\n",
    "   p_spam_given_message = ProbabilitySpam\n",
    "   p_ham_given_message = ProbabilityHam\n",
    "\n",
    "   for word in message:\n",
    "      if word in parameters_spam:\n",
    "         p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "      if word in parameters_ham:\n",
    "         p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "   if p_ham_given_message > p_spam_given_message:\n",
    "      #return 'ham'\n",
    "        #print(\"It is a ham mail.\")\n",
    "        return 0\n",
    "   elif p_spam_given_message > p_ham_given_message:\n",
    "      #return 'spam'\n",
    "        #print(\"It is a spam mail.\")\n",
    "        return 1\n",
    "   else:\n",
    "        #print(\"It is a ham mail.\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['predicted'] = test_set['text'].apply(classify_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "report = classification_report(test_set['label_nums'],test_set['predicted'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_test_set('Our way of saying Thanks for being a part of our community! Building the tools to support and inspire active creators like you is what keeps us going at VSCO.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_test_set(\"Spank monster daddy is on discount only and only just for you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "67cc67b46fd777197e0ed19b9bc9b40b00abaa09652c221dae7491e11f3cd393"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
